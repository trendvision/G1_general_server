{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GoogleScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleScraper import scrape_with_config, GoogleSearchError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GoogleScraper.\n",
    "# print(starting_url)\n",
    "# &recent=7D\n",
    "# //*[@id=\"uniq15882370222762553381\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainThread] - 2020-05-05 12:03:46,122 - GoogleScraperG1.core - INFO - Going to scrape 1 keywords with 1 proxies by using 1 threads.\n",
      "[MainThread] - 2020-05-05 12:03:46,123 - GoogleScraperG1.scraping - INFO - [+] GoogleSelScrape[localhost][search-type:image][https://www.google.com/search?] using search engine \"google\". Num keywords=1, num pages for keyword=[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_base_url https://www.google.com/search?\n",
      "specific_base_url https://www.google.com/search?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.google.com/imghp?tbm=isch\n",
      "\n",
      "\n",
      "\n",
      "QUERY reddit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"d1c6d944cd5e1957cd6e304604e192ae\", element=\"1562c815-6081-43fe-89e4-7772e0789d09\")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-5] - 2020-05-05 12:03:50,509 - GoogleScraperG1.scraping - INFO - [GoogleSelScrape-google][localhost]]Keyword: \"reddit\" with [1] pages, slept 1 seconds before scraping. 1/1 already scraped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m1/1 keywords processed.\u001b[0m\r",
      "[i] Going to scrape 0 images and saving them in \"images/\"\n"
     ]
    }
   ],
   "source": [
    "from GoogleScraperG1 import scrape_with_config, GoogleSearchError\n",
    "target_directory = 'images/'\n",
    "\n",
    "# See in the config.cfg file for possible values\n",
    "config = {\n",
    "#     'use_own_ip': False,\n",
    "#     \"proxy_file\": \"GoogleScraperG1/proxies.txt\",\n",
    "#     \"proxy\": \"http 174.138.42.112:8080\",\n",
    "#     \"proxy\": 1,\n",
    "    'keyword': 'reddit', # :D hehe have fun my dear friends\n",
    "#     'search_engines': ['yandex', 'google', 'bing', 'yahoo'], # duckduckgo not supported\n",
    "#     'search_engines': ['yandex'], # duckduckgo not supported\n",
    "    'search_engines': ['google'], # duckduckgo not supported\n",
    "    'num_pages_for_keyword': 1,\n",
    "    'search_type': 'image',\n",
    "    \n",
    "\n",
    "    \"raise_exceptions_while_scraping\": False,\n",
    "    \n",
    "    \n",
    "#     'scrape_method': 'http',\n",
    "\n",
    "    'scrape_method': 'selenium',\n",
    "    'sel_browser': 'chrome',\n",
    "    \"browser-mode\": \"headless\",\n",
    "    \"chromedriver_path\": \"chromedriver\",    \n",
    "    'do_caching': False,\n",
    "    'output_filename': 'out.csv',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    search = scrape_with_config(config)\n",
    "except GoogleSearchError as e:\n",
    "    print(e)\n",
    "\n",
    "image_urls = []\n",
    "\n",
    "for serp in search.serps:\n",
    "    image_urls.extend(\n",
    "        [link.link for link in serp.links]\n",
    "    )\n",
    "\n",
    "print('[i] Going to scrape {num} images and saving them in \"{dir}\"'.format(\n",
    "    num=len(image_urls),\n",
    "    dir=target_directory\n",
    "))\n",
    "\n",
    "import threading,requests, os, urllib\n",
    "\n",
    "# In our case we want to download the\n",
    "# images as fast as possible, so we use threads.\n",
    "class FetchResource(threading.Thread):\n",
    "    \"\"\"Grabs a web resource and stores it in the target directory.\n",
    "    Args:\n",
    "        target: A directory where to save the resource.\n",
    "        urls: A bunch of urls to grab\n",
    "    \"\"\"\n",
    "    def __init__(self, target, urls):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.urls = urls\n",
    "\n",
    "    def run(self):\n",
    "        for url in self.urls:\n",
    "            url = urllib.parse.unquote(url)\n",
    "            with open(os.path.join(self.target, url.split('/')[-1]), 'wb') as f:\n",
    "                try:\n",
    "                    content = requests.get(url).content\n",
    "                    f.write(content)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                print('[+] Fetched {}'.format(url))\n",
    "\n",
    "# make a directory for the results\n",
    "try:\n",
    "    os.mkdir(target_directory)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# fire up 100 threads to get the images\n",
    "num_threads = 100\n",
    "\n",
    "threads = [FetchResource('images/'+\"data\", []) for i in range(num_threads)]\n",
    "\n",
    "\n",
    "while image_urls:\n",
    "    for t in threads:\n",
    "        try:\n",
    "            t.urls.append(image_urls.pop())\n",
    "        except IndexError as e:\n",
    "            break\n",
    "\n",
    "threads = [t for t in threads if t.urls]\n",
    "\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mGoogleScraper.zip\u001b[0m     \u001b[01;34m__MACOSX\u001b[0m/        google_scraper.db  \u001b[01;34mtelega\u001b[0m/\r\n",
      "\u001b[01;34mGoogleScraperG1\u001b[0m/      chromedriver     googlescraper.log\r\n",
      "Untitled-Copy1.ipynb  geckodriver      \u001b[01;34mimages\u001b[0m/\r\n",
      "Untitled.ipynb        geckodriver.log  out.csv\r\n"
     ]
    }
   ],
   "source": [
    "https 107.190.148.202:50854\n",
    "https 216.228.69.202:54085\n",
    "https 173.217.255.36:33351\n",
    "https 35.233.5.198:3128\n",
    "https 35.222.208.56:3128\n",
    "https 65.152.119.226:56455\n",
    "https 173.46.67.172:58517\n",
    "https 69.65.65.178:34548\n",
    "https 104.45.188.43:3128\n",
    "https 192.41.71.204:3128\n",
    "https 192.92.205.132:80\n",
    "https 216.198.188.26:51068\n",
    "https 192.41.13.71:3128\n",
    "https 192.41.71.221:3128\n",
    "https 158.222.90.4:8083\n",
    "https 158.222.90.10:8081\n",
    "https 158.222.90.11:8081\n",
    "https 158.222.90.2:8080\n",
    "https 45.63.108.68:33938\n",
    "http 174.138.42.112:8080\n",
    "http 34.73.222.183:8080\n",
    "http 12.139.101.100:80\n",
    "http 157.245.15.86:8080\n",
    "http 63.134.172.12:80\n",
    "http 178.128.159.243:5836\n",
    "http 35.238.211.255:8080\n",
    "http 50.0.51.253:46917\n",
    "http 159.65.181.107:3128\n",
    "http 209.239.112.213:9191\n",
    "http 44.232.52.177:8090\n",
    "http 69.64.54.93:9191\n",
    "http 69.64.54.66:9191\n",
    "http 34.90.88.37:8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
